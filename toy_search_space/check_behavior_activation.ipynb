{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8d667a-7c2d-446d-9d1c-78f88d982773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dbc913-5e26-4837-aca0-c9511adb355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearOp(torch.nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super(LinearOp, self).__init__()\n",
    "        self.linop = torch.nn.Linear(in_dim,out_dim)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.act(self.linop(x))\n",
    "    \n",
    "class ConvOp3x3(torch.nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super(ConvOp3x3, self).__init__()\n",
    "        self.convop = torch.nn.Conv2d(in_channel,out_channel,3,stride=6)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.in_channel = in_channel\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_res = int(np.sqrt(x.shape[-1]//self.in_channel))\n",
    "        x = x.reshape(x.shape[0], self.in_channel, x_res,x_res)\n",
    "        return self.act(self.convop(x)).reshape(x.shape[0],-1)\n",
    "\n",
    "class ConvOp1x1(torch.nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super(ConvOp1x1, self).__init__()\n",
    "        self.convop = torch.nn.Conv2d(in_channel,out_channel,1,stride=6)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.in_channel = in_channel\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_res = int(np.sqrt(x.shape[-1]//self.in_channel))\n",
    "        x = x.reshape(x.shape[0], self.in_channel, x_res,x_res)\n",
    "        return self.act(self.convop(x)).reshape(x.shape[0],-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b432f3-69da-4d62-a51e-904a6200a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32,3*16*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac761e1a-b9cd-4d70-99c9-704ac11ad37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a095933d-c2b8-4bb3-b8b9-2a265968bac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 9])\n",
      "torch.Size([32, 9])\n",
      "torch.Size([32, 9])\n"
     ]
    }
   ],
   "source": [
    "ops = [LinearOp(3*16*16,9), ConvOp3x3(3,1), ConvOp1x1(3,1)]\n",
    "for op in ops:\n",
    "    out = op(x)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7337bef-b82a-4506-9f30-3b3aba7ae226",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsemax = Sparsemax(dim=-1)\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "alphas1 = softmax(torch.randn([2]))\n",
    "alphas2 = softmax(torch.randn([2]))\n",
    "weights = alphas1.reshape(alphas1.shape[0], 1) @ alphas2.reshape(1, alphas2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6cdb57-8aad-4e66-b8cd-360857fa8e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(weights.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82867f98-bb10-4b0b-97fd-6660d56dfa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1849, 0.8151])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e61729d-d930-430c-b43b-fe65eb4427a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas1 = torch.nn.Parameter(1e-3*torch.ones([3]))\n",
    "alphas2 = torch.nn.Parameter(1e-3*torch.ones([3]))\n",
    "softmax = torch.nn.Softmax(dim=-1) \n",
    "alphas1 = softmax(alphas1)\n",
    "alphas2 = softmax(alphas2)\n",
    "weights = (alphas1.reshape(alphas1.shape[0], 1) @ alphas2.reshape(1, alphas2.shape[0])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7951c6b7-9736-404b-8653-e589526b894c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83156547-e974-433c-a749-eafb0ec09f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixOp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MixOp, self).__init__()\n",
    "        self.alphas1 = torch.nn.Parameter(1e-3*torch.ones([3]))\n",
    "        self.alphas2 = torch.nn.Parameter(1e-3*torch.ones([3]))\n",
    "        self.softmax = torch.nn.Softmax(dim=-1) \n",
    "        \n",
    "    def forward(self,x, op_list1,op_list2):\n",
    "        s = 0\n",
    "        alphas1 = self.softmax(self.alphas1)\n",
    "        alphas2 = self.softmax(self.alphas2)\n",
    "        weights = (alphas1.reshape(alphas1.shape[0], 1) @ alphas2.reshape(1, alphas2.shape[0])).flatten()\n",
    "        #print(weights)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                #print(op_list1[i](x).shape)\n",
    "                #print(op_list2[i](x).shape)\n",
    "                s = s+ weights[i+j]*(op_list1[i](x)+op_list2[j](x))\n",
    "                #print(s.shape)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80c4c82d-8a91-41df-bd95-ffd001d646b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1036.2162, grad_fn=<SumBackward0>)\n",
      "tensor([ 80.0731,   8.8275, -88.9007])\n",
      "tensor([-1.0173e-05, -1.0173e-05, -1.0173e-05])\n",
      "tensor(955.7281, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.5009, -0.2884, -0.2125])\n",
      "tensor([ 0.0011,  0.0042, -0.0053])\n",
      "tensor(921.7460, grad_fn=<SumBackward0>)\n",
      "tensor([ 1.5235, -0.9867, -0.5368])\n",
      "tensor([-0.0141,  0.0059,  0.0082])\n",
      "tensor(907.5768, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.2932, -0.1982, -0.0950])\n",
      "tensor([ 0.0028, -0.0003, -0.0026])\n",
      "tensor(887.7828, grad_fn=<SumBackward0>)\n",
      "tensor([-0.1487,  0.1567, -0.0079])\n",
      "tensor([ 0.0394,  0.0410, -0.0804])\n",
      "tensor(911.4223, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.3269, -0.2405, -0.0865])\n",
      "tensor([ 0.0094, -0.0026, -0.0068])\n",
      "tensor(905.4025, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.2122, -0.1538, -0.0584])\n",
      "tensor([ 0.0134,  0.0084, -0.0218])\n",
      "tensor(921.5274, grad_fn=<SumBackward0>)\n",
      "tensor([-0.2715,  0.2114,  0.0601])\n",
      "tensor([-0.0038, -0.0024,  0.0062])\n",
      "tensor(897.8585, grad_fn=<SumBackward0>)\n",
      "tensor([-0.1993,  0.1603,  0.0390])\n",
      "tensor([-0.0039,  0.0021,  0.0018])\n",
      "tensor(935.5146, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.2272, -0.1894, -0.0379])\n",
      "tensor([-0.0023, -0.0067,  0.0090])\n",
      "tensor(957.6761, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.2096, -0.1680, -0.0416])\n",
      "tensor([ 0.0144,  0.0078, -0.0222])\n",
      "tensor(899.7562, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0308, -0.0235, -0.0073])\n",
      "tensor([ 0.0046,  0.0047, -0.0093])\n",
      "tensor(948.0731, grad_fn=<SumBackward0>)\n",
      "tensor([-0.1537,  0.1313,  0.0224])\n",
      "tensor([ 0.0054,  0.0011, -0.0066])\n",
      "tensor(954.4557, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0814,  0.0729,  0.0085])\n",
      "tensor([ 0.0100,  0.0064, -0.0164])\n",
      "tensor(901.3165, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.1315, -0.1107, -0.0208])\n",
      "tensor([ 0.0124, -0.0071, -0.0053])\n",
      "tensor(913.5896, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.1482, -0.1332, -0.0150])\n",
      "tensor([-0.0159, -0.0160,  0.0319])\n",
      "tensor(933.6918, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0448,  0.0497, -0.0049])\n",
      "tensor([ 0.0257,  0.0275, -0.0533])\n",
      "tensor(878.1001, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0109, -0.0089, -0.0020])\n",
      "tensor([ 0.0006,  0.0009, -0.0016])\n",
      "tensor(884.3570, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0810,  0.0765,  0.0045])\n",
      "tensor([ 0.0241,  0.0106, -0.0347])\n",
      "tensor(901.1523, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0536, -0.0382, -0.0154])\n",
      "tensor([ 0.0189,  0.0163, -0.0352])\n",
      "tensor(925.1027, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0978,  0.0843,  0.0135])\n",
      "tensor([ 0.0036,  0.0006, -0.0042])\n",
      "tensor(890.0331, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0425,  0.0355,  0.0070])\n",
      "tensor([-0.0020, -0.0008,  0.0028])\n",
      "tensor(969.2576, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0578, -0.0426, -0.0151])\n",
      "tensor([ 0.0155,  0.0158, -0.0314])\n",
      "tensor(914.7237, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0447, -0.0335, -0.0112])\n",
      "tensor([ 0.0200,  0.0009, -0.0210])\n",
      "tensor(906.5095, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0504,  0.0414,  0.0090])\n",
      "tensor([-0.0079,  0.0018,  0.0061])\n",
      "tensor(908.6497, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0156,  0.0115,  0.0041])\n",
      "tensor([-0.0038, -0.0043,  0.0081])\n",
      "tensor(917.7567, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0043,  0.0026,  0.0017])\n",
      "tensor([-0.0061,  0.0009,  0.0052])\n",
      "tensor(915.2072, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0302, -0.0257, -0.0045])\n",
      "tensor([-4.9863e-04,  7.5990e-05,  4.2264e-04])\n",
      "tensor(889.4344, grad_fn=<SumBackward0>)\n",
      "tensor([ 4.6846e-03,  7.0362e-05, -4.7550e-03])\n",
      "tensor([ 0.0081,  0.0116, -0.0197])\n",
      "tensor(929.3370, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0121,  0.0097,  0.0024])\n",
      "tensor([-0.0028,  0.0004,  0.0024])\n",
      "tensor(938.2495, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0156, -0.0136, -0.0020])\n",
      "tensor([-0.0016, -0.0005,  0.0021])\n",
      "tensor(915.4432, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0282, -0.0246, -0.0035])\n",
      "tensor([-0.0053,  0.0007,  0.0046])\n",
      "tensor(909.1769, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0469,  0.0395,  0.0074])\n",
      "tensor([-2.1736e-04,  3.1064e-05,  1.8630e-04])\n",
      "tensor(949.6740, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0113,  0.0079,  0.0034])\n",
      "tensor([-0.0091,  0.0012,  0.0079])\n",
      "tensor(894.8142, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0229,  0.0202,  0.0027])\n",
      "tensor([ 0.0051, -0.0007, -0.0044])\n",
      "tensor(952.4109, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0150,  0.0126,  0.0024])\n",
      "tensor([ 2.8190e-05, -3.7104e-06, -2.4480e-05])\n",
      "tensor(921.0984, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0304,  0.0254,  0.0050])\n",
      "tensor([-6.5774e-04,  8.5588e-05,  5.7215e-04])\n",
      "tensor(920.3191, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0109, -0.0085, -0.0024])\n",
      "tensor([ 0.0035, -0.0005, -0.0031])\n",
      "tensor(930.7164, grad_fn=<SumBackward0>)\n",
      "tensor([ 2.4783e-04, -5.5694e-05, -1.9214e-04])\n",
      "tensor([ 0.0009, -0.0001, -0.0008])\n",
      "tensor(910.6539, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0060,  0.0045,  0.0016])\n",
      "tensor([-0.0033,  0.0004,  0.0029])\n",
      "tensor(913.0703, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0045,  0.0027,  0.0019])\n",
      "tensor([-0.0064,  0.0008,  0.0056])\n",
      "tensor(894.9218, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0247, -0.0219, -0.0028])\n",
      "tensor([-0.0067,  0.0008,  0.0059])\n",
      "tensor(880.5933, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0121, -0.0093, -0.0029])\n",
      "tensor([ 0.0041,  0.0006, -0.0047])\n",
      "tensor(927.3187, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0157,  0.0126,  0.0032])\n",
      "tensor([-0.0038,  0.0004,  0.0034])\n",
      "tensor(968.1044, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0093,  0.0075,  0.0019])\n",
      "tensor([-0.0021,  0.0002,  0.0018])\n",
      "tensor(934.6575, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0043, -0.0032, -0.0011])\n",
      "tensor([ 0.0023, -0.0003, -0.0021])\n",
      "tensor(946.5955, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0286,  0.0228,  0.0058])\n",
      "tensor([-0.0087,  0.0028,  0.0059])\n",
      "tensor(915.4009, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0148,  0.0114,  0.0034])\n",
      "tensor([-0.0047, -0.0005,  0.0052])\n",
      "tensor(939.9438, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0020,  0.0023, -0.0003])\n",
      "tensor([ 0.0033, -0.0004, -0.0030])\n",
      "tensor(886.0549, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0125,  0.0108,  0.0017])\n",
      "tensor([-0.0011,  0.0026, -0.0016])\n",
      "tensor(919.2942, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0020, -0.0003, -0.0017])\n",
      "tensor([ 6.6386e-03,  9.1524e-05, -6.7301e-03])\n",
      "tensor(928.7943, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0053,  0.0044,  0.0009])\n",
      "tensor([-5.6348e-04,  6.8864e-05,  4.9462e-04])\n",
      "tensor(914.9639, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0098, -0.0061, -0.0037])\n",
      "tensor([ 0.0124, -0.0016, -0.0108])\n",
      "tensor(898.8810, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0130,  0.0085,  0.0045])\n",
      "tensor([-0.0142,  0.0018,  0.0123])\n",
      "tensor(907.9069, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0038, -0.0029, -0.0008])\n",
      "tensor([ 0.0013, -0.0002, -0.0012])\n",
      "tensor(909.1132, grad_fn=<SumBackward0>)\n",
      "tensor([-9.7431e-03,  9.7572e-03, -1.4009e-05])\n",
      "tensor([ 0.0089, -0.0012, -0.0077])\n",
      "tensor(923.2139, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0071,  0.0068,  0.0003])\n",
      "tensor([ 0.0051, -0.0011, -0.0041])\n",
      "tensor(913.8732, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0069, -0.0073,  0.0004])\n",
      "tensor([-0.0083,  0.0011,  0.0072])\n",
      "tensor(936.6537, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0034,  0.0037, -0.0003])\n",
      "tensor([ 0.0047, -0.0006, -0.0041])\n",
      "tensor(941.7274, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0182, -0.0149, -0.0033])\n",
      "tensor([ 0.0023, -0.0003, -0.0020])\n",
      "tensor(920.0268, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0283, -0.0220, -0.0063])\n",
      "tensor([ 0.0063,  0.0021, -0.0083])\n",
      "tensor(900.9287, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0211,  0.0213, -0.0002])\n",
      "tensor([ 0.0107,  0.0075, -0.0182])\n",
      "tensor(877.4504, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0247,  0.0206,  0.0041])\n",
      "tensor([-2.7468e-04,  3.4021e-05,  2.4066e-04])\n",
      "tensor(884.9628, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0070, -0.0046, -0.0024])\n",
      "tensor([ 0.0072, -0.0009, -0.0063])\n",
      "tensor(892.6882, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0161, -0.0137, -0.0024])\n",
      "tensor([-0.0033,  0.0020,  0.0013])\n",
      "tensor(900.9274, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0164,  0.0141,  0.0023])\n",
      "tensor([ 0.0008,  0.0013, -0.0021])\n",
      "tensor(888.6213, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0256,  0.0199,  0.0057])\n",
      "tensor([-0.0085,  0.0014,  0.0071])\n",
      "tensor(923.1798, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0070,  0.0056,  0.0013])\n",
      "tensor([-0.0009,  0.0001,  0.0008])\n",
      "tensor(927.4019, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0182, -0.0149, -0.0033])\n",
      "tensor([ 0.0017, -0.0002, -0.0015])\n",
      "tensor(941.0396, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0017,  0.0006, -0.0023])\n",
      "tensor([ 0.0071,  0.0028, -0.0098])\n",
      "tensor(921.9028, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0067,  0.0056,  0.0011])\n",
      "tensor([ 2.6507e-04, -3.0166e-05, -2.3490e-04])\n",
      "tensor(924.9329, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0049, -0.0032, -0.0017])\n",
      "tensor([ 0.0049, -0.0006, -0.0044])\n",
      "tensor(898.9570, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0097,  0.0085,  0.0012])\n",
      "tensor([ 0.0022, -0.0003, -0.0019])\n",
      "tensor(899.3779, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0034, -0.0036,  0.0002])\n",
      "tensor([-0.0041,  0.0005,  0.0036])\n",
      "tensor(924.9385, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0077,  0.0062,  0.0015])\n",
      "tensor([-0.0009,  0.0001,  0.0008])\n",
      "tensor(931.3670, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0030,  0.0021,  0.0009])\n",
      "tensor([-0.0004, -0.0014,  0.0018])\n",
      "tensor(924.2677, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0003, -0.0009,  0.0005])\n",
      "tensor([-0.0032,  0.0004,  0.0028])\n",
      "tensor(903.3051, grad_fn=<SumBackward0>)\n",
      "tensor([-8.3226e-05, -9.7407e-04,  1.0573e-03])\n",
      "tensor([-0.0069,  0.0019,  0.0050])\n",
      "tensor(926.0344, grad_fn=<SumBackward0>)\n",
      "tensor([-7.2665e-04,  4.8399e-05,  6.7825e-04])\n",
      "tensor([-0.0046,  0.0019,  0.0027])\n",
      "tensor(904.7806, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0081,  0.0058,  0.0023])\n",
      "tensor([-0.0028, -0.0016,  0.0044])\n",
      "tensor(950.1014, grad_fn=<SumBackward0>)\n",
      "tensor([ 6.6735e-05, -9.1742e-05,  2.5008e-05])\n",
      "tensor([-2.0328e-04,  2.6454e-05,  1.7682e-04])\n",
      "tensor(915.6407, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0045, -0.0035, -0.0010])\n",
      "tensor([ 0.0030, -0.0020, -0.0011])\n",
      "tensor(902.9753, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0056,  0.0021,  0.0034])\n",
      "tensor([-0.0142,  0.0019,  0.0123])\n",
      "tensor(899.8013, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0061, -0.0048, -0.0013])\n",
      "tensor([ 0.0015, -0.0002, -0.0013])\n",
      "tensor(937.1972, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0006,  0.0005, -0.0011])\n",
      "tensor([ 0.0053, -0.0007, -0.0046])\n",
      "tensor(895.4635, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0027, -0.0024, -0.0003])\n",
      "tensor([-0.0008,  0.0001,  0.0006])\n",
      "tensor(868.9990, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0078, -0.0054, -0.0024])\n",
      "tensor([ 0.0059, -0.0008, -0.0052])\n",
      "tensor(929.6111, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0174, -0.0148, -0.0026])\n",
      "tensor([-0.0005, -0.0013,  0.0019])\n",
      "tensor(918.0663, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0114,  0.0096,  0.0018])\n",
      "tensor([-0.0010,  0.0017, -0.0006])\n",
      "tensor(949.7594, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0006,  0.0013, -0.0006])\n",
      "tensor([ 0.0041, -0.0005, -0.0036])\n",
      "tensor(924.3894, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0140,  0.0117,  0.0023])\n",
      "tensor([-0.0003,  0.0009, -0.0006])\n",
      "tensor(923.8022, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0041,  0.0037,  0.0005])\n",
      "tensor([ 0.0014, -0.0002, -0.0012])\n",
      "tensor(929.3699, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0090, -0.0070, -0.0019])\n",
      "tensor([ 0.0020, -0.0003, -0.0017])\n",
      "tensor(929.2563, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0044, -0.0030, -0.0014])\n",
      "tensor([ 0.0030, -0.0004, -0.0027])\n",
      "tensor(934.6874, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0029,  0.0026,  0.0002])\n",
      "tensor([ 0.0015, -0.0002, -0.0014])\n",
      "tensor(940.9265, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0043, -0.0036, -0.0007])\n",
      "tensor([-6.5792e-04,  7.3332e-05,  5.8458e-04])\n",
      "tensor(903.0656, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0010,  0.0021, -0.0011])\n",
      "tensor([ 0.0068, -0.0008, -0.0060])\n",
      "tensor(929.7498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0062,  0.0047,  0.0015])\n",
      "tensor([-0.0019,  0.0002,  0.0017])\n",
      "tensor(956.2145, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0030, -0.0036,  0.0006])\n",
      "tensor([-0.0057,  0.0006,  0.0052])\n",
      "tensor(890.7865, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0138,  0.0111,  0.0027])\n",
      "tensor([-0.0010,  0.0004,  0.0006])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ops1 = [LinearOp(3*16*16,9), ConvOp3x3(3,1), ConvOp1x1(3,1)]\n",
    "ops2 = [LinearOp(3*16*16,9), ConvOp3x3(3,1), ConvOp1x1(3,1)]\n",
    "mixop = MixOp()\n",
    "optimizer_weights = torch.optim.SGD(list(ops1[0].parameters())+list(ops1[1].parameters())+list(ops1[2].parameters())+list(ops2[0].parameters())+list(ops2[1].parameters())+list(ops2[2].parameters()),0.0001)\n",
    "optimizer_arch = torch.optim.Adam(list(mixop.parameters()),0.001) #+list(mixop.parameters())\n",
    "for epoch in range(100000):\n",
    "    x = torch.randn(128,3*16*16)\n",
    "    y = torch.randn(128,9)\n",
    "    out = mixop(x,ops1,ops2)\n",
    "    optimizer_weights.zero_grad()\n",
    "    loss_main= torch.sum(torch.abs(out-y))\n",
    "    loss_main.backward()\n",
    "    optimizer_weights.step()\n",
    "    x = torch.randn(128,3*16*16)\n",
    "    y = torch.randn(128,9)\n",
    "    out = mixop(x,ops1,ops2)\n",
    "    optimizer_arch.zero_grad()\n",
    "    loss_arch = torch.sum(torch.abs(out-y))\n",
    "    loss_arch.backward()\n",
    "    optimizer_arch.step()\n",
    "    if epoch%1000==0:\n",
    "        print(loss_main)\n",
    "        print(mixop.alphas1.grad)\n",
    "        print(mixop.alphas2.grad)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10865933-d3fa-4792-8a38-9e52966f137b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(922.3684, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3a1c33b-0e1a-4de9-b726-1f8ea3762e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0008, 0.8133, 0.1858], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.0430, 0.1040, 0.8531], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(mixop.alphas1,dim=-1))\n",
    "print(torch.softmax(mixop.alphas2,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3be6cd2b-a0f4-4198-a1c2-11bb9222867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixOpDiscretize(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MixOpDiscretize, self).__init__()\n",
    "        self.softmax = torch.nn.Softmax(dim=-1) \n",
    "        \n",
    "    def forward(self,x, op1,op2):\n",
    "        s =op1(x)+op2(x)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f538effa-3ea6-410c-ad3f-ed28554d3b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(917.3347, grad_fn=<SumBackward0>)\n",
      "(0, 0)\n",
      "tensor(892.6663, grad_fn=<SumBackward0>)\n",
      "(0, 1)\n",
      "tensor(932.1953, grad_fn=<SumBackward0>)\n",
      "(0, 2)\n",
      "tensor(915.5466, grad_fn=<SumBackward0>)\n",
      "(1, 0)\n",
      "tensor(936.4576, grad_fn=<SumBackward0>)\n",
      "(1, 1)\n",
      "tensor(926.5770, grad_fn=<SumBackward0>)\n",
      "(1, 2)\n",
      "tensor(920.6604, grad_fn=<SumBackward0>)\n",
      "(2, 0)\n",
      "tensor(946.0222, grad_fn=<SumBackward0>)\n",
      "(2, 1)\n",
      "tensor(937.4080, grad_fn=<SumBackward0>)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "ops1 = [LinearOp(3*16*16,9), ConvOp3x3(3,1), ConvOp1x1(3,1)]\n",
    "ops2 = [LinearOp(3*16*16,9), ConvOp3x3(3,1), ConvOp1x1(3,1)]\n",
    "mixop = MixOpDiscretize()\n",
    "index1 = [0,1,2]\n",
    "index2 = [0,1,2]\n",
    "losses = {}\n",
    "for i in index1:\n",
    "    for j in index2:\n",
    "        optimizer_weights = torch.optim.SGD(list(ops1[i].parameters())+list(ops2[j].parameters()),0.00001)\n",
    "        for epoch in range(100000):\n",
    "            x = torch.randn(128,3*16*16)\n",
    "            y = torch.randn(128,9)\n",
    "            out = mixop(x,ops1[i],ops2[j])\n",
    "            optimizer_weights.zero_grad()\n",
    "            loss_main= torch.sum(torch.abs(out-y))\n",
    "            loss_main.backward()\n",
    "            optimizer_weights.step()\n",
    "        print(loss_main)\n",
    "        print((i,j))\n",
    "        losses[(i,j)]=loss_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffe7e005-82cc-4704-8d3a-e6c88722d4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): tensor(917.3347, grad_fn=<SumBackward0>), (0, 1): tensor(892.6663, grad_fn=<SumBackward0>), (0, 2): tensor(932.1953, grad_fn=<SumBackward0>), (1, 0): tensor(915.5466, grad_fn=<SumBackward0>), (1, 1): tensor(936.4576, grad_fn=<SumBackward0>), (1, 2): tensor(926.5770, grad_fn=<SumBackward0>), (2, 0): tensor(920.6604, grad_fn=<SumBackward0>), (2, 1): tensor(946.0222, grad_fn=<SumBackward0>), (2, 2): tensor(937.4080, grad_fn=<SumBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38af5f69-a637-4d3c-94ca-9f9d9023ec1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msum(mixop(x,\u001b[43mops1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex1\u001b[49m\u001b[43m]\u001b[49m,ops2[index2]))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "x = torch.randn(64,3*16*16)\n",
    "torch.sum(mixop(x,ops1[index1],ops2[index2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "56303c57-27f7-4494-92e1-02e64c5e760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4a61a7d2-e6d8-4812-ab25-157129c02fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MixOp' object has no attribute 'alphas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [287]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmixop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malphas\u001b[49m\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/anaconda3/envs/autoformer_py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MixOp' object has no attribute 'alphas'"
     ]
    }
   ],
   "source": [
    "mixop.alphas.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1c6551db-a8fb-4549-962c-6dd7d0c648ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2205, 0.0024, 0.7771])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "param = torch.Tensor([1,2,3])\n",
    "print(param)\n",
    "m = torch.distributions.dirichlet.Dirichlet(softmax(param))\n",
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3bea13-6d98-4f17-8346-ce18d7574e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sparsemax activation function.\n",
    "\n",
    "Pytorch implementation of Sparsemax function from:\n",
    "-- \"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\"\n",
    "-- André F. T. Martins, Ramón Fernandez Astudillo (http://arxiv.org/abs/1602.02068)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "    \"\"\"Sparsemax function.\"\"\"\n",
    "\n",
    "    def __init__(self, dim=None):\n",
    "        \"\"\"Initialize sparsemax activation\n",
    "        \n",
    "        Args:\n",
    "            dim (int, optional): The dimension over which to apply the sparsemax function.\n",
    "        \"\"\"\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor. First dimension should be the batch size\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size x number_of_logits] Output tensor\n",
    "\n",
    "        \"\"\"\n",
    "        # Sparsemax currently only handles 2-dim tensors,\n",
    "        # so we reshape to a convenient shape and reshape back after sparsemax\n",
    "        input = input.transpose(0, self.dim)\n",
    "        original_size = input.size()\n",
    "        input = input.reshape(input.size(0), -1)\n",
    "        input = input.transpose(0, 1)\n",
    "        dim = 1\n",
    "\n",
    "        number_of_logits = input.size(dim)\n",
    "\n",
    "        # Translate input by max for numerical stability\n",
    "        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n",
    "\n",
    "        # Sort input in descending order.\n",
    "        # (NOTE: Can be replaced with linear time selection method described here:\n",
    "        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n",
    "        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n",
    "        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n",
    "        range = range.expand_as(zs)\n",
    "\n",
    "        # Determine sparsity of projection\n",
    "        bound = 1 + range * zs\n",
    "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
    "        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n",
    "        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n",
    "\n",
    "        # Compute threshold function\n",
    "        zs_sparse = is_gt * zs\n",
    "\n",
    "        # Compute taus\n",
    "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
    "        taus = taus.expand_as(input)\n",
    "\n",
    "        # Sparsemax\n",
    "        self.output = torch.max(torch.zeros_like(input), input - taus)\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        output = self.output\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.reshape(original_size)\n",
    "        output = output.transpose(0, self.dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Backward function.\"\"\"\n",
    "        dim = 1\n",
    "\n",
    "        nonzeros = torch.ne(self.output, 0)\n",
    "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
    "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
    "\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a26e939-f98f-4407-a8ab-a3dcc96776fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "beta = F.elu(1e-3*torch.randn([3])) + 1\n",
    "weights1 = torch.distributions.dirichlet.Dirichlet(beta).rsample()\n",
    "weights2 = torch.distributions.dirichlet.Dirichlet(beta).rsample()\n",
    "weights = (weights1.reshape(weights1.shape[0], 1) @ weights2.reshape(1, weights2.shape[0])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4376c09f-7578-4bd7-ae9c-0522670d3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0283, 0.0028, 0.0410, 0.3423, 0.0333, 0.4958, 0.0222, 0.0022, 0.0322])\n",
      "tensor(1.0000)\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor([0.5010, 0.0250, 0.0831, 0.0719, 0.0036, 0.0119, 0.2496, 0.0125, 0.0414])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0076, 0.2136, 0.0120, 0.0184, 0.5157, 0.0291, 0.0066, 0.1865, 0.0105])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0050, 0.0097, 0.0053, 0.0034, 0.0068, 0.0037, 0.2390, 0.4700, 0.2570])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3173, 0.1596, 0.0278, 0.2738, 0.1377, 0.0240, 0.0377, 0.0189, 0.0033])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0581, 0.1019, 0.0147, 0.1802, 0.3159, 0.0456, 0.0943, 0.1653, 0.0239])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0661, 0.0590, 0.1330, 0.1340, 0.1198, 0.2697, 0.0559, 0.0500, 0.1125])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0109, 0.0095, 0.0297, 0.0838, 0.0732, 0.2286, 0.1226, 0.1071, 0.3346])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0272, 0.0712, 0.0865, 0.0749, 0.1959, 0.2381, 0.0451, 0.1178, 0.1432])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0138, 0.0022, 0.0029, 0.1629, 0.0254, 0.0340, 0.5561, 0.0867, 0.1161])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([6.7470e-01, 8.1892e-03, 4.1777e-02, 2.4615e-01, 2.9876e-03, 1.5241e-02,\n",
      "        1.0201e-02, 1.2382e-04, 6.3164e-04])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0069, 0.0414, 0.0071, 0.0438, 0.2612, 0.0450, 0.0743, 0.4437, 0.0765])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n",
      "tensor([0.1978, 0.0304, 0.0512, 0.2609, 0.0401, 0.0676, 0.2491, 0.0383, 0.0645])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0802, 0.3208, 0.0808, 0.0477, 0.1908, 0.0481, 0.0385, 0.1541, 0.0388])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0417, 0.0049, 0.3422, 0.0236, 0.0028, 0.1937, 0.0420, 0.0049, 0.3442])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1649, 0.0117, 0.0870, 0.0998, 0.0071, 0.0527, 0.3610, 0.0255, 0.1905])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0076, 0.0200, 0.0096, 0.1554, 0.4075, 0.1951, 0.0420, 0.1101, 0.0527])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0285, 0.5043, 0.1177, 0.0008, 0.0135, 0.0031, 0.0145, 0.2575, 0.0601])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1307, 0.0967, 0.0158, 0.4028, 0.2979, 0.0486, 0.0040, 0.0030, 0.0005])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1101, 0.4740, 0.3104, 0.0111, 0.0477, 0.0312, 0.0019, 0.0082, 0.0054])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0274, 0.0156, 0.0082, 0.3401, 0.1936, 0.1016, 0.1678, 0.0955, 0.0501])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0123, 0.7091, 0.0598, 0.0014, 0.0828, 0.0070, 0.0020, 0.1158, 0.0098])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([1.4163e-03, 5.3541e-02, 4.7997e-01, 4.2699e-04, 1.6141e-02, 1.4470e-01,\n",
      "        8.0440e-04, 3.0408e-02, 2.7260e-01])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0073, 0.0750, 0.0725, 0.0124, 0.1274, 0.1233, 0.0275, 0.2818, 0.2728])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3071, 0.0780, 0.0436, 0.0498, 0.0126, 0.0071, 0.3594, 0.0912, 0.0511])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0135, 0.0702, 0.0358, 0.0667, 0.3460, 0.1762, 0.0330, 0.1713, 0.0873])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0801, 0.0838, 0.0524, 0.2122, 0.2220, 0.1387, 0.0781, 0.0817, 0.0511])\n",
      "tensor(1.0000)\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor([0.1064, 0.0705, 0.0175, 0.3595, 0.2380, 0.0593, 0.0815, 0.0539, 0.0134])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0028, 0.0194, 0.0669, 0.0025, 0.0172, 0.0593, 0.0260, 0.1812, 0.6247])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0210, 0.0024, 0.0322, 0.2473, 0.0285, 0.3795, 0.1091, 0.0126, 0.1674])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1089, 0.5842, 0.1641, 0.0041, 0.0220, 0.0062, 0.0140, 0.0752, 0.0211])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3072, 0.0378, 0.3732, 0.0763, 0.0094, 0.0927, 0.0442, 0.0054, 0.0537])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3915, 0.1735, 0.2457, 0.0712, 0.0316, 0.0447, 0.0202, 0.0090, 0.0127])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1334, 0.0649, 0.0639, 0.0740, 0.0360, 0.0355, 0.3013, 0.1466, 0.1445])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0004, 0.1694, 0.0423, 0.0008, 0.3149, 0.0786, 0.0008, 0.3145, 0.0785])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0737, 0.0426, 0.2707, 0.0113, 0.0066, 0.0416, 0.1054, 0.0610, 0.3871])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1471, 0.0365, 0.1035, 0.1283, 0.0319, 0.0903, 0.2369, 0.0588, 0.1667])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0258, 0.0172, 0.0242, 0.1260, 0.0837, 0.1183, 0.2324, 0.1543, 0.2180])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0498, 0.0917, 0.0453, 0.1543, 0.2841, 0.1405, 0.0624, 0.1150, 0.0569])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2317, 0.1667, 0.2819, 0.0785, 0.0565, 0.0955, 0.0304, 0.0219, 0.0370])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1655, 0.3581, 0.0150, 0.0416, 0.0901, 0.0038, 0.1001, 0.2167, 0.0091])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.4841, 0.1072, 0.2543, 0.0659, 0.0146, 0.0346, 0.0224, 0.0050, 0.0118])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0325, 0.0558, 0.0513, 0.0912, 0.1566, 0.1440, 0.1091, 0.1872, 0.1722])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([5.6078e-03, 4.0802e-04, 8.7614e-03, 4.4840e-02, 3.2626e-03, 7.0057e-02,\n",
      "        3.2904e-01, 2.3941e-02, 5.1408e-01])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2057, 0.0571, 0.0582, 0.3003, 0.0834, 0.0849, 0.1348, 0.0374, 0.0381])\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor([0.3076, 0.0778, 0.0026, 0.1171, 0.0296, 0.0010, 0.3681, 0.0931, 0.0032])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n",
      "tensor([0.0828, 0.2234, 0.0155, 0.0853, 0.2303, 0.0159, 0.0892, 0.2409, 0.0167])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0999, 0.0135, 0.0790, 0.3601, 0.0487, 0.2849, 0.0591, 0.0080, 0.0468])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0046, 0.0015, 0.0028, 0.0641, 0.0204, 0.0393, 0.4488, 0.1433, 0.2752])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0009, 0.0062, 0.0032, 0.0323, 0.2241, 0.1180, 0.0531, 0.3683, 0.1939])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n",
      "tensor([0.1267, 0.1081, 0.2721, 0.1174, 0.1001, 0.2521, 0.0059, 0.0050, 0.0126])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1068, 0.2444, 0.0326, 0.1645, 0.3763, 0.0502, 0.0070, 0.0160, 0.0021])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0055, 0.0046, 0.0616, 0.0100, 0.0084, 0.1118, 0.0610, 0.0514, 0.6857])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2129, 0.0843, 0.2088, 0.0193, 0.0076, 0.0189, 0.1885, 0.0747, 0.1849])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3432, 0.0504, 0.0161, 0.2434, 0.0358, 0.0114, 0.2510, 0.0369, 0.0118])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0059, 0.0307, 0.0140, 0.0099, 0.0521, 0.0237, 0.1001, 0.5246, 0.2389])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0426, 0.3910, 0.3570, 0.0061, 0.0562, 0.0513, 0.0052, 0.0473, 0.0432])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2980, 0.2704, 0.0214, 0.1224, 0.1110, 0.0088, 0.0849, 0.0770, 0.0061])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0691, 0.0415, 0.5341, 0.0203, 0.0122, 0.1570, 0.0178, 0.0107, 0.1375])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1306, 0.1124, 0.2177, 0.0675, 0.0581, 0.1124, 0.0854, 0.0735, 0.1424])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3033, 0.0379, 0.5226, 0.0279, 0.0035, 0.0480, 0.0200, 0.0025, 0.0344])\n",
      "tensor(1.0000)\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor([0.0832, 0.2102, 0.1792, 0.0764, 0.1930, 0.1645, 0.0164, 0.0416, 0.0354])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0568, 0.1703, 0.1981, 0.0534, 0.1600, 0.1862, 0.0234, 0.0702, 0.0817])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0016, 0.0043, 0.0034, 0.0424, 0.1157, 0.0911, 0.1262, 0.3443, 0.2709])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1296, 0.0714, 0.1365, 0.0103, 0.0057, 0.0109, 0.2441, 0.1345, 0.2571])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1065, 0.0559, 0.0931, 0.0075, 0.0039, 0.0065, 0.3029, 0.1590, 0.2647])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1470, 0.1172, 0.0817, 0.1402, 0.1118, 0.0779, 0.1378, 0.1099, 0.0766])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0158, 0.0084, 0.0155, 0.2160, 0.1151, 0.2116, 0.1663, 0.0886, 0.1629])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0037, 0.0611, 0.0352, 0.0112, 0.1870, 0.1077, 0.0218, 0.3631, 0.2092])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0549, 0.0616, 0.0154, 0.2913, 0.3269, 0.0815, 0.0701, 0.0786, 0.0196])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1780, 0.3016, 0.0061, 0.0351, 0.0595, 0.0012, 0.1534, 0.2599, 0.0052])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1566, 0.0493, 0.1247, 0.0257, 0.0081, 0.0205, 0.2915, 0.0917, 0.2321])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2785, 0.0382, 0.1226, 0.2634, 0.0361, 0.1160, 0.0920, 0.0126, 0.0405])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0797, 0.0186, 0.0172, 0.0767, 0.0179, 0.0166, 0.5334, 0.1247, 0.1151])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([1.4385e-01, 5.7276e-01, 3.1791e-02, 4.9941e-04, 1.9885e-03, 1.1037e-04,\n",
      "        4.7860e-02, 1.9056e-01, 1.0577e-02])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2294, 0.0062, 0.0319, 0.1170, 0.0032, 0.0163, 0.5111, 0.0139, 0.0711])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0016, 0.0144, 0.0305, 0.0242, 0.2210, 0.4677, 0.0082, 0.0746, 0.1578])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.5159, 0.0100, 0.0228, 0.2932, 0.0057, 0.0129, 0.1313, 0.0025, 0.0058])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0142, 0.1462, 0.2401, 0.0086, 0.0892, 0.1466, 0.0126, 0.1296, 0.2129])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1018, 0.1058, 0.0501, 0.2507, 0.2607, 0.1234, 0.0425, 0.0442, 0.0209])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0117, 0.0623, 0.2623, 0.0153, 0.0813, 0.3422, 0.0078, 0.0417, 0.1753])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1658, 0.2471, 0.3938, 0.0235, 0.0351, 0.0559, 0.0162, 0.0241, 0.0385])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3443, 0.2584, 0.0719, 0.0135, 0.0101, 0.0028, 0.1526, 0.1145, 0.0319])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0212, 0.0147, 0.2506, 0.0107, 0.0074, 0.1264, 0.0421, 0.0292, 0.4975])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1549, 0.0818, 0.0508, 0.0888, 0.0468, 0.0291, 0.2952, 0.1558, 0.0968])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n",
      "tensor([0.0020, 0.0043, 0.0008, 0.1779, 0.3791, 0.0744, 0.1018, 0.2170, 0.0426])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0445, 0.0126, 0.0082, 0.3397, 0.0963, 0.0630, 0.2966, 0.0841, 0.0550])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0768, 0.0648, 0.0639, 0.0367, 0.0309, 0.0305, 0.2602, 0.2195, 0.2166])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.1128, 0.0478, 0.1038, 0.3030, 0.1284, 0.2790, 0.0107, 0.0046, 0.0099])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0729, 0.0450, 0.1356, 0.1135, 0.0700, 0.2110, 0.1013, 0.0625, 0.1883])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0022, 0.0429, 0.0121, 0.0152, 0.3022, 0.0852, 0.0205, 0.4054, 0.1143])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0081, 0.0256, 0.0197, 0.0045, 0.0144, 0.0111, 0.1388, 0.4390, 0.3387])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.3791, 0.1008, 0.0233, 0.1143, 0.0304, 0.0070, 0.2600, 0.0692, 0.0160])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0140, 0.0050, 0.0083, 0.4960, 0.1775, 0.2931, 0.0031, 0.0011, 0.0019])\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2551, 0.1979, 0.1814, 0.1184, 0.0918, 0.0842, 0.0286, 0.0222, 0.0204])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0016, 0.1274, 0.1282, 0.0008, 0.0642, 0.0646, 0.0037, 0.3038, 0.3058])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0023, 0.0485, 0.0676, 0.0117, 0.2432, 0.3395, 0.0056, 0.1175, 0.1640])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.0278, 0.0446, 0.2022, 0.0389, 0.0623, 0.2828, 0.0346, 0.0554, 0.2513])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor([0.2292, 0.2081, 0.0055, 0.2431, 0.2207, 0.0058, 0.0453, 0.0411, 0.0011])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n",
      "tensor([8.2093e-04, 1.5463e-01, 5.2110e-01, 6.0651e-05, 1.1424e-02, 3.8499e-02,\n",
      "        3.3183e-04, 6.2503e-02, 2.1063e-01])\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    weights1 = torch.distributions.dirichlet.Dirichlet(beta).rsample()\n",
    "    weights2 = torch.distributions.dirichlet.Dirichlet(beta).rsample()\n",
    "    weights = (weights1.reshape(weights1.shape[0], 1) @ weights2.reshape(1, weights2.shape[0])).flatten()    \n",
    "    print(weights)\n",
    "    print(sum(weights))\n",
    "    print(sum(weights1))\n",
    "    print(sum(weights2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c5e45d84-3347-4338-a058-0cec9bc59d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gumbel test\n",
    "param_vector = torch.nn.Parameter(1e-3*torch.randn([3]))\n",
    "\n",
    "def sample_gumbel(n,k):\n",
    "    unif = torch.distributions.Uniform(0,1).sample((n,k))\n",
    "    g = -torch.log(-torch.log(unif))\n",
    "    return g\n",
    "def sample_gumbel_softmax(pi, n=1, temperature=0.1):\n",
    "    k = pi.shape[0]\n",
    "    print(pi)\n",
    "    g = sample_gumbel(n, k)\n",
    "    print(g)\n",
    "    h = (g + torch.log(pi))/temperature\n",
    "    h_max = h.max(dim=1, keepdim=True)[0]\n",
    "    h = h - h_max\n",
    "    cache = torch.exp(h)\n",
    "    y = cache / cache.sum(dim=-1, keepdim=True)\n",
    "    print(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c7f7ca96-f1ad-44c6-9874-1bbd0bd84270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3336, 0.3332, 0.3333], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0.4600, -0.3800, -0.0432]])\n",
      "tensor([[0.2780, 0.3008, 0.4213]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "param_vector_soft = torch.nn.functional.softmax(param_vector, dim = -1)\n",
    "loss = torch.sum(sample_gumbel_softmax(param_vector_soft,temperature=1)*torch.ones([3]))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8d1580e7-ab9f-4884-b9d7-193e5ae8f515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(param_vector.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b944e24-7553-4232-92ba-0a14948d6750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n",
      "torch.Size([64, 4, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvOp5x5(torch.nn.Module):\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super(ConvOp5x5, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=2,padding=2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, affine=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.bn(self.conv(x))\n",
    "    \n",
    "class ConvOp3x3(torch.nn.Module):\n",
    "    def __init__(self,Conv5x5):\n",
    "        super(ConvOp3x3, self).__init__()\n",
    "        self.conv_base = Conv5x5.conv\n",
    "        self.bn = Conv5x5.bn\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.bn(F.conv2d(x, self.conv_base.weight[:,:,1:4,1:4],bias=self.conv_base.bias, stride=2,padding=1))\n",
    "    \n",
    "class ConvOp1x1(torch.nn.Module):\n",
    "    def __init__(self,Conv5x5):\n",
    "        super(ConvOp1x1, self).__init__()\n",
    "        self.conv_base = Conv5x5.conv\n",
    "        self.bn = Conv5x5.bn\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.bn(F.conv2d(x, self.conv_base.weight[:,:,2:3,2:3],bias=self.conv_base.bias, stride=2))\n",
    "\n",
    "class DWSConv7x7(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DWSConv7x7, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=2, padding=3, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, affine=True)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return self.bn(out)\n",
    "    \n",
    "class DWSConv5x5(nn.Module):\n",
    "    def __init__(self, DWSConv7x7):\n",
    "        super(DWSConv5x5, self).__init__()\n",
    "        self.model = DWSConv7x7\n",
    "        self.depthwise_base = DWSConv7x7.depthwise #nn.Conv2d(in_channels, in_channels, kernel_size=5, stride=2, padding=2, groups=in_channels)\n",
    "        self.pointwise_base = DWSConv7x7.pointwise #(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = DWSConv7x7.bn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.conv2d(x, self.depthwise_base.weight[:,:,1:6,1:6],bias=self.depthwise_base.bias, stride=2,padding=2,groups=self.model.in_channels)\n",
    "        out = F.conv2d(out, self.pointwise_base.weight,bias=self.pointwise_base.bias)\n",
    "        return self.bn(out)\n",
    "    \n",
    "class DWSConv3x3(nn.Module):\n",
    "    def __init__(self, DWSConv7x7):\n",
    "        super(DWSConv3x3, self).__init__()\n",
    "        self.model = DWSConv7x7\n",
    "        self.depthwise_base = DWSConv7x7.depthwise #nn.Conv2d(in_channels, in_channels, kernel_size=5, stride=2, padding=2, groups=in_channels)\n",
    "        self.pointwise_base = DWSConv7x7.pointwise #(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = DWSConv7x7.bn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.conv2d(x, self.depthwise_base.weight[:,:,2:5,2:5],bias=self.depthwise_base.bias, stride=2,padding=1,groups=self.model.in_channels)\n",
    "        out = F.conv2d(out, self.pointwise_base.weight,bias=self.pointwise_base.bias)\n",
    "        return self.bn(out)\n",
    "    \n",
    "class ConvMaxPool5x5(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvMaxPool5x5, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.pool(self.conv(x))) \n",
    "        \n",
    "class ConvMaxPool3x3(nn.Module):\n",
    "    def __init__(self, ConvMaxPool5x5):\n",
    "        super(ConvMaxPool3x3, self).__init__()\n",
    "        self.conv = ConvMaxPool5x5.conv #nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1, padding=1)\n",
    "        self.pool = ConvMaxPool5x5.pool #nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.bn = ConvMaxPool5x5.bn #nn.BatchNorm2d(out_channels, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.pool(F.conv2d(x, self.conv.weight[:,:,1:4,1:4], bias=self.conv.bias, stride=1, padding=1)))\n",
    "    \n",
    "class ConvMaxPool1x1(nn.Module):\n",
    "    def __init__(self, ConvMaxPool5x5):\n",
    "        super(ConvMaxPool1x1, self).__init__()\n",
    "        self.conv = ConvMaxPool5x5.conv #nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1, padding=1)\n",
    "        self.pool = ConvMaxPool5x5.pool #nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.bn = ConvMaxPool5x5.bn #nn.BatchNorm2d(out_channels, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.pool(F.conv2d(x, self.conv.weight[:,:,2:3,2:3], bias=self.conv.bias, stride=1)))\n",
    "\n",
    "class ConvAvgPool5x5(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvAvgPool5x5, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1, padding=2)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.pool(self.conv(x))) \n",
    "        \n",
    "class ConvAvgPool3x3(nn.Module):\n",
    "    def __init__(self, ConvAvgPool5x5):\n",
    "        super(ConvAvgPool3x3, self).__init__()\n",
    "        self.conv = ConvAvgPool5x5.conv #nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1, padding=1)\n",
    "        self.pool = ConvAvgPool5x5.pool #nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.bn = ConvAvgPool5x5.bn #nn.BatchNorm2d(out_channels, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.pool(F.conv2d(x, self.conv.weight[:,:,1:4,1:4], bias=self.conv.bias, stride=1, padding=1)))\n",
    "    \n",
    "class ConvAvgPool1x1(nn.Module):\n",
    "    def __init__(self, ConvAvgPool5x5):\n",
    "        super(ConvAvgPool1x1, self).__init__()\n",
    "        self.conv = ConvAvgPool5x5.conv #nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1, padding=1)\n",
    "        self.pool = ConvAvgPool5x5.pool #nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.bn = ConvAvgPool5x5.bn #nn.BatchNorm2d(out_channels, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.pool(F.conv2d(x, self.conv.weight[:,:,2:3,2:3], bias=self.conv.bias, stride=1)))\n",
    "\n",
    "    \n",
    "class DilConv5x5(nn.Module):\n",
    "    \n",
    "    def __init__(self, C_in, C_out):\n",
    "        super(DilConv5x5, self).__init__()\n",
    "        self.op = nn.Sequential(\n",
    "         nn.Conv2d(C_in, C_in, kernel_size=5, stride=1, padding=1, dilation=4, groups=C_in, bias=False),\n",
    "         nn.Conv2d(C_in, C_out, kernel_size=1, stride=1, bias=False),\n",
    "         nn.BatchNorm2d(C_out, affine=True)\n",
    "         )\n",
    "        self.C_in = C_in\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "    \n",
    "    \n",
    "class DilConv3x3(nn.Module):\n",
    "    \n",
    "    def __init__(self, DilConv5x5):\n",
    "        super(DilConv3x3, self).__init__()\n",
    "        self.model = DilConv5x5\n",
    "        self.conv1 = DilConv5x5.op[0]\n",
    "        self.conv2 = DilConv5x5.op[1]\n",
    "        self.bn = DilConv5x5.op[2]\n",
    "    def forward(self, x):\n",
    "        out = F.conv2d(x, self.conv1.weight[:,:,1:4,1:4], bias=self.conv1.bias, stride=2, padding=2, dilation=2, groups = self.model.C_in)\n",
    "        out = F.conv2d(out, self.conv2.weight, bias=self.conv2.bias, stride=1)\n",
    "        return self.bn(out)\n",
    "\n",
    "\n",
    "    \n",
    "class FactorizedReduce(nn.Module):\n",
    "\n",
    "    def __init__(self, C_in, C_out):\n",
    "        super(FactorizedReduce, self).__init__()\n",
    "        assert C_out % 2 == 0\n",
    "        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n",
    "        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False) \n",
    "        self.bn = nn.BatchNorm2d(C_out, affine=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)\n",
    "        return self.bn(out)\n",
    "    \n",
    "    \n",
    "a = torch.randn([64,3,28,28])\n",
    "op1 = ConvOp5x5(3,4)\n",
    "print(op1(a).shape)\n",
    "op2 = ConvOp3x3(op1)\n",
    "print(op2(a).shape)\n",
    "op3 = ConvOp1x1(op1)\n",
    "print(op3(a).shape)\n",
    "op4 = DWSConv7x7(3,4)\n",
    "print(op4(a).shape)\n",
    "op5 = DWSConv5x5(op4)\n",
    "print(op5(a).shape)\n",
    "op6 = DWSConv3x3(op4)\n",
    "print(op6(a).shape)\n",
    "op7 = ConvMaxPool5x5(3,4)\n",
    "print(op7(a).shape)\n",
    "op8 = ConvMaxPool3x3(op7)\n",
    "print(op8(a).shape)\n",
    "op9 = ConvMaxPool1x1(op8)\n",
    "print(op9(a).shape)\n",
    "op7 = ConvAvgPool5x5(3,4)\n",
    "print(op7(a).shape)\n",
    "op8 = ConvAvgPool3x3(op7)\n",
    "print(op8(a).shape)\n",
    "op9 = ConvAvgPool1x1(op8)\n",
    "print(op9(a).shape)\n",
    "op10 = DilConv5x5(3,4)\n",
    "print(op10(a).shape)\n",
    "op11 = DilConv3x3(op10)\n",
    "print(op11(a).shape)\n",
    "op12 = FactorizedReduce(3,4)\n",
    "print(op12(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a49b24-35da-4ab0-a2b5-eef32178e728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
